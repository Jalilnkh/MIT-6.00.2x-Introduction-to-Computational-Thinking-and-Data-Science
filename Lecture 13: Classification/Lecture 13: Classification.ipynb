{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 13: Classification \n",
    "Description: Prof. Guttag introduces supervised learning with nearest neighbor classification using feature scaling and decision trees.\n",
    "\n",
    "Instructor: John Guttag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-nearest neighbors\n",
    "The k-nearest neighbors (k-NN) algorithm is a simple, intuitive, and widely-used machine learning algorithm for classification and regression tasks. It is a type of instance-based learning, which means it doesn't construct a general internal model but instead relies on storing instances of the training data.\n",
    "\n",
    "### How k-NN Works\n",
    "\n",
    "1. **Data Storage**:\n",
    "   - The algorithm stores all the training data points. Each data point has features (independent variables) and a label (dependent variable).\n",
    "\n",
    "2. **Choosing k**:\n",
    "   - `k` is a user-defined constant that specifies the number of nearest neighbors to consider. Common choices for `k` are small positive integers like 3, 5, or 7, but it can be any positive integer.\n",
    "\n",
    "3. **Distance Metric**:\n",
    "   - A distance metric is used to measure the similarity between data points. The most common metric is Euclidean distance, but others like Manhattan distance, Minkowski distance, or Hamming distance can also be used.\n",
    "\n",
    "4. **Making Predictions**:\n",
    "   - **For Classification**:\n",
    "     - To classify a new data point, the algorithm calculates the distance from this point to all training data points.\n",
    "     - It then selects the `k` nearest neighbors (data points with the smallest distances).\n",
    "     - The new data point is assigned the most common class (majority vote) among its `k` nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of k-Nearest Neighbors (k-NN)\n",
    "\n",
    "1. **Simplicity**:\n",
    "   - k-NN is easy to understand and implement. It doesn't require any complex parameter tuning or model building.\n",
    "\n",
    "2. **No Training Phase**:\n",
    "   - Since k-NN is a lazy learning algorithm, it doesn't have a training phase. This makes it fast to set up and start making predictions.\n",
    "\n",
    "3. **Adaptability**:\n",
    "   - k-NN can be used for both classification and regression tasks.\n",
    "\n",
    "4. **No Assumptions about Data**:\n",
    "   - k-NN makes no assumptions about the underlying data distribution, making it a non-parametric method.\n",
    "\n",
    "5. **Flexibility with Distance Metrics**:\n",
    "   - Various distance metrics can be employed, allowing customization based on the specific problem (e.g., Euclidean, Manhattan, Minkowski, Hamming distances).\n",
    "\n",
    "### Disadvantages of k-Nearest Neighbors (k-NN)\n",
    "\n",
    "1. **Computational Complexity**:\n",
    "   - k-NN can be computationally expensive, especially for large datasets, as it requires calculating the distance between the new data point and all existing points in the dataset.\n",
    "\n",
    "2. **Storage Requirement**:\n",
    "   - The algorithm needs to store all the training data, which can be memory-intensive.\n",
    "\n",
    "3. **Sensitive to Irrelevant Features**:\n",
    "   - k-NN's performance can degrade if the data contains irrelevant or redundant features. Feature scaling and selection are often necessary.\n",
    "\n",
    "4. **Curse of Dimensionality**:\n",
    "   - As the number of features increases, the distance between data points becomes less informative (a phenomenon known as the curse of dimensionality). This can lead to decreased performance.\n",
    "\n",
    "5. **Choosing the Right k**:\n",
    "   - Selecting the optimal value of `k` can be tricky. A small `k` can lead to noisy predictions (overfitting), while a large `k` can smooth out the decision boundary too much (underfitting).\n",
    "\n",
    "6. **Imbalanced Data**:\n",
    "   - k-NN can perform poorly on imbalanced datasets because the algorithm might be biased towards the more frequent class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.86\n",
      "Recall: 0.76\n",
      "Accuracy: 0.81\n",
      "F1 Score: 0.81\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(78.8, 13.0, 83.9, 24.3)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from lecturer_code import random_splits, knn_classification\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "examples = list(zip(X, y))\n",
    "\n",
    "# Run the k-NN classification with random splits\n",
    "random_splits(examples, knn_classification, num_splits=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omniaz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
